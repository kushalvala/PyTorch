{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-NLP-01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqimindJ7A9r",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "When dealing with a natural language processing task, we take a text corpus and break it down into smaller units. We will break the sentences down into individual words, where each word represents a meaning along with the other words in its proximity to convey the intent of a sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCcauA7DMa9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using lambda function to create a tokenizer\n",
        "\n",
        "tokenizer = lambda x : x.split()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91LMfZpZ9VMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b257a77b-b2fe-49eb-d1ae-6f2905a14936"
      },
      "source": [
        "tokenizer('This is a test string')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'test', 'string']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnbG_iZN-U0C",
        "colab_type": "text"
      },
      "source": [
        "We tokenized the sentences with spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny_8YGpG_XzX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "76ed44fb-1383-4be0-cb0b-3b7c7a347788"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LpCo6yV9yNb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ce5d553-ba22-447f-d4df-cf08c8c93cd3"
      },
      "source": [
        "# Alternative Approach: using nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokenize(\"This is a, test string\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', ',', 'test', 'string']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZs1r2uG_1zX",
        "colab_type": "text"
      },
      "source": [
        "### Creating Fields\n",
        "\n",
        "Fields let us define the datatype and help us create tensors out of textual data by specifying the set of operations to be performed on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApM-fA1B_U-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field\n",
        "\n",
        "# Fields for Reviews\n",
        "Review = Field(sequential= True, tokenize= tokenizer , lower= True)\n",
        "\n",
        "# Fields for Labels\n",
        "Label = Field(sequential= False, use_vocab= False)\n",
        "\n",
        "# Adding token at end and starting of the input string\n",
        "\n",
        "SequenceField = Field(tokenize= tokenizer, init_token= '<sos>', eos_token= '<sos>', lower= True)\n",
        "\n",
        "# Setting a field with a fix length\n",
        "\n",
        "SequenceField = Field(tokenize= tokenizer, init_token= '<sos>', eos_token= '<sos>', lower= True, fix_length= 50)\n",
        "\n",
        "# Setting an unknown token\n",
        "\n",
        "SequenceField = Field(tokenize= tokenizer, init_token= '<sos>', eos_token= '<sos>', lower= True, unk_token= '<unk>')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTfznYjX86Br",
        "colab_type": "text"
      },
      "source": [
        "### Developing the Dataset\n",
        "\n",
        "TorchText can read data from text files, CSV/TSV files, JSON files, and directories and converts them into a dataset. Datasets are preprocessed blocks of data that are read into memory, and can be used by other data structures. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytZxyhkhDGfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import TabularDataset\n",
        "\n",
        "# Selecting the training columns\n",
        "train_datafields = [(\"id\", None), (\"content\", Review), (\"Business\", Label), (\"SciTech\", Label), (\"Sports\", Label), (\"World\", Label)]\n",
        "\n",
        "# Selecting the testing columns\n",
        "test_datafields = [('id',None), ('content',Review)]\n",
        "\n",
        "# Reading the training and validation file\n",
        "train, valid = TabularDataset.splits(path= '/content/NewsClassificaiton' ,train='train.csv', validation='valid.csv', format='csv', skip_header=True, fields=train_datafields)\n",
        "\n",
        "# Reading the test file\n",
        "\n",
        "test = TabularDataset(path= '/content/NewsClassificaiton/test.csv', format = 'csv', skip_header= True, fields= test_datafields)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibg-cs-yBn2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building the vocabulary\n",
        "Review = Field(sequential= True, tokenize= tokenizer , lower= True)\n",
        "\n",
        "Review.build_vocab(train, min_freq=2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTZ6lGc_EOjC",
        "colab_type": "text"
      },
      "source": [
        "We used the TabularDataset module in torchtext to read the CSV file, which can also be used to read inputs in the TSV, JSON, and Python dictionaries, which define a dataset of columns.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnCi4eFLEa9M",
        "colab_type": "text"
      },
      "source": [
        "### Developing Iterators\n",
        "\n",
        "Iterators are used to load batches of data from the dataset. They provide methods to make loading data and moving data to the appropriate device easier. We could use these iterator objects to iterate over the data while running through the epochs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPAIiOf_C5Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import BucketIterator\n",
        "import torch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLmTmozsErP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the Batch Size\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iter, valid_iter, test_iter = BucketIterator.splits((train, valid, test),\n",
        "                                     batch_size=BATCH_SIZE,\n",
        "                                     device=device,\n",
        "                                     sort_key=lambda x: len(x.comment_text), \n",
        "                                     sort_within_batch=False)"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}